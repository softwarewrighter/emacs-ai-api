# LiteLLM Gateway Configuration
# Unified endpoint for all LLM providers (cloud + local)
# Documentation: https://docs.litellm.ai/docs/proxy/config_settings

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL
  ui: true
  store_model_in_db: true
  
  # Optional alerting (uncomment to enable)
  # alerting:
  #   - webhook_url: https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK
  #     alerting_threshold: 300  # seconds

litellm_settings:
  timeout: 120
  num_retries: 2
  drop_params: true
  # Cache settings (optional)
  # cache: true
  # cache_params:
  #   type: redis
  #   host: redis
  #   port: 6379
  #   password: os.environ/REDIS_PASSWORD

router_settings:
  # Routing strategy options:
  # - latency-based-routing (fastest response)
  # - cost-based-routing (cheapest provider)
  # - usage-based-routing (least used)
  # - simple-shuffle (random)
  routing_strategy: latency-based-routing
  
  # Provider-level budgets (reset on time_period)
  provider_budget_config:
    openai:
      budget_limit: 200.0     # $/period
      time_period: 30d        # 1d/7d/30d/1mo
    anthropic:
      budget_limit: 150.0
      time_period: 30d
    deepseek:
      budget_limit: 100.0
      time_period: 30d
    google:
      budget_limit: 100.0
      time_period: 30d
    openrouter:
      budget_limit: 50.0
      time_period: 30d

model_list:
  # ========== DIRECT MODEL ACCESS ==========
  
  # OpenAI Models
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
      max_budget: 10.0
      budget_duration: 1d
  
  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
      max_budget: 5.0
      budget_duration: 1d
  
  - model_name: gpt-4-turbo
    litellm_params:
      model: openai/gpt-4-turbo
      api_key: os.environ/OPENAI_API_KEY
      max_budget: 8.0
      budget_duration: 1d
  
  - model_name: o1-preview
    litellm_params:
      model: openai/o1-preview
      api_key: os.environ/OPENAI_API_KEY
      max_budget: 15.0
      budget_duration: 1d
  
  - model_name: o1-mini
    litellm_params:
      model: openai/o1-mini
      api_key: os.environ/OPENAI_API_KEY
      max_budget: 5.0
      budget_duration: 1d
  
  # Anthropic Models
  - model_name: claude-3-5-sonnet
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
      max_budget: 10.0
      budget_duration: 1d
  
  - model_name: claude-3-5-haiku
    litellm_params:
      model: anthropic/claude-3-5-haiku-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
      max_budget: 5.0
      budget_duration: 1d
  
  - model_name: claude-3-opus
    litellm_params:
      model: anthropic/claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY
      max_budget: 15.0
      budget_duration: 1d
  
  # DeepSeek Models
  - model_name: deepseek-chat
    litellm_params:
      model: deepseek/deepseek-chat
      api_key: os.environ/DEEPSEEK_API_KEY
      api_base: https://api.deepseek.com/v1
      max_budget: 5.0
      budget_duration: 1d
  
  - model_name: deepseek-coder
    litellm_params:
      model: deepseek/deepseek-coder
      api_key: os.environ/DEEPSEEK_API_KEY
      api_base: https://api.deepseek.com/v1
      max_budget: 5.0
      budget_duration: 1d

  # Z.ai (Zhipu AI) Models - Using Coding Plan endpoint (quota-based, not per-token)
  # The "secret": use /api/coding/paas/v4 instead of /api/paas/v4
  - model_name: glm-4.7
    litellm_params:
      model: openai/glm-4.7
      api_key: os.environ/ZAI_API_KEY
      api_base: https://api.z.ai/api/coding/paas/v4
      max_budget: 5.0
      budget_duration: 1d

  - model_name: glm-4.5-flash
    litellm_params:
      model: openai/glm-4.5-flash
      api_key: os.environ/ZAI_API_KEY
      api_base: https://api.z.ai/api/coding/paas/v4
      max_budget: 3.0
      budget_duration: 1d

  # Z.ai metered API (pay-per-token, for comparison)
  - model_name: glm-4.7-metered
    litellm_params:
      model: zai/glm-4.7
      api_key: os.environ/ZAI_API_KEY
      max_budget: 5.0
      budget_duration: 1d

  # Google Gemini Models (supports both GEMINI_API_KEY and GOOGLE_API_KEY)
  - model_name: gemini-1.5-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: os.environ/GEMINI_API_KEY
      max_budget: 8.0
      budget_duration: 1d

  - model_name: gemini-1.5-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GEMINI_API_KEY
      max_budget: 3.0
      budget_duration: 1d

  - model_name: gemini-2.0-flash-exp
    litellm_params:
      model: gemini/gemini-2.0-flash-exp
      api_key: os.environ/GEMINI_API_KEY
      max_budget: 5.0
      budget_duration: 1d
  
  # OpenRouter Models (future)
  - model_name: openrouter-auto
    litellm_params:
      model: openrouter/auto
      api_base: https://openrouter.ai/api/v1
      api_key: os.environ/OPENROUTER_API_KEY
      max_budget: 5.0
      budget_duration: 1d
  
  # ========== LOCAL MODELS ==========
  
  # Local llama.cpp server
  - model_name: local-deepseek
    litellm_params:
      model: deepseek-coder-v2
      api_base: http://llama:8080/v1
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
  
  - model_name: local-kimi
    litellm_params:
      model: kimi-k2
      api_base: http://llama:8080/v1
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
  
  # Local Ollama server (host machine)
  - model_name: ollama-deepseek
    litellm_params:
      model: ollama/deepseek-coder:latest
      api_base: http://host.docker.internal:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
  
  - model_name: ollama-qwen
    litellm_params:
      model: ollama/qwen2.5-coder:7b
      api_base: http://host.docker.internal:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
  
  - model_name: ollama-llama
    litellm_params:
      model: ollama/llama3.2:latest
      api_base: http://host.docker.internal:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  # ========== LAN OLLAMA SERVERS ==========
  # Manager (localhost via Docker host)
  - model_name: manager-qwen14b
    litellm_params:
      model: ollama/qwen2.5-coder:14b
      api_base: http://host.docker.internal:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  - model_name: manager-gemma9b
    litellm_params:
      model: ollama/gemma2:9b
      api_base: http://host.docker.internal:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  # Big72 server
  - model_name: big72-qwen14b
    litellm_params:
      model: ollama/qwen2.5-coder:14b
      api_base: http://big72.local:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  - model_name: big72-mistral7b
    litellm_params:
      model: ollama/mistral:7b
      api_base: http://big72.local:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  # Curiosity server
  - model_name: curiosity-qwen14b
    litellm_params:
      model: ollama/qwen2.5-coder:14b
      api_base: http://curiosity.local:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  - model_name: curiosity-gemma9b
    litellm_params:
      model: ollama/gemma2:9b
      api_base: http://curiosity.local:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  # Hive server
  - model_name: hive-qwen14b
    litellm_params:
      model: ollama/qwen2.5-coder:14b
      api_base: http://hive.local:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  - model_name: hive-gemma9b
    litellm_params:
      model: ollama/gemma2:9b
      api_base: http://hive.local:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  # ========== LOCAL-ONLY ROUTING (load balanced across LAN) ==========

  # Local Root LLM - larger models for RLM protocol (14B+)
  # LiteLLM will try each in order, failover if unavailable
  - model_name: local-root
    litellm_params:
      model: ollama/qwen2.5-coder:14b
      api_base: http://host.docker.internal:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
  - model_name: local-root
    litellm_params:
      model: ollama/qwen2.5-coder:14b
      api_base: http://big72.local:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
  - model_name: local-root
    litellm_params:
      model: ollama/qwen2.5-coder:14b
      api_base: http://curiosity.local:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
  - model_name: local-root
    litellm_params:
      model: ollama/qwen2.5-coder:14b
      api_base: http://hive.local:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  # Local Sub LLM - smaller models for sub-tasks (7-9B)
  - model_name: local-sub
    litellm_params:
      model: ollama/gemma2:9b
      api_base: http://host.docker.internal:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
  - model_name: local-sub
    litellm_params:
      model: ollama/gemma2:9b
      api_base: http://big72.local:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
  - model_name: local-sub
    litellm_params:
      model: ollama/mistral:7b
      api_base: http://curiosity.local:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
  - model_name: local-sub
    litellm_params:
      model: ollama/gemma2:9b
      api_base: http://hive.local:11434
      api_key: sk-local
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  # ========== ROUTING ALIASES ==========
  # Same model_name repeated = load balancing/fallback
  
  # Best quality (cloud providers, GPT-4/Claude priority)
  - model_name: coding-best
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
  - model_name: coding-best
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
  - model_name: coding-best
    litellm_params:
      model: openai/o1-preview
      api_key: os.environ/OPENAI_API_KEY
  
  # Balanced (good quality, moderate cost)
  - model_name: coding-balanced
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
  - model_name: coding-balanced
    litellm_params:
      model: anthropic/claude-3-5-haiku-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
  - model_name: coding-balanced
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GEMINI_API_KEY
  - model_name: coding-balanced
    litellm_params:
      model: deepseek/deepseek-coder
      api_key: os.environ/DEEPSEEK_API_KEY
      api_base: https://api.deepseek.com/v1
  
  # Cheap (local first, then low-cost cloud)
  - model_name: coding-cheap
    litellm_params:
      model: deepseek-coder-v2
      api_base: http://llama:8080/v1
      api_key: sk-local
  - model_name: coding-cheap
    litellm_params:
      model: ollama/deepseek-coder:latest
      api_base: http://host.docker.internal:11434
      api_key: sk-local
  - model_name: coding-cheap
    litellm_params:
      model: deepseek/deepseek-coder
      api_key: os.environ/DEEPSEEK_API_KEY
      api_base: https://api.deepseek.com/v1
  - model_name: coding-cheap
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GEMINI_API_KEY
  
  # Auto-router (mix of all available)
  - model_name: coding-auto
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
  - model_name: coding-auto
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
  - model_name: coding-auto
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: os.environ/GEMINI_API_KEY
  - model_name: coding-auto
    litellm_params:
      model: deepseek/deepseek-coder
      api_key: os.environ/DEEPSEEK_API_KEY
      api_base: https://api.deepseek.com/v1
  - model_name: coding-auto
    litellm_params:
      model: deepseek-coder-v2
      api_base: http://llama:8080/v1
      api_key: sk-local
  
  # Reasoning specialist (o1 models)
  - model_name: reasoning
    litellm_params:
      model: openai/o1-preview
      api_key: os.environ/OPENAI_API_KEY
  - model_name: reasoning
    litellm_params:
      model: openai/o1-mini
      api_key: os.environ/OPENAI_API_KEY
  - model_name: reasoning
    litellm_params:
      model: anthropic/claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY