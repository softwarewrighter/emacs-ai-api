version: "3.8"

services:
  postgres:
    image: postgres:16
    container_name: litellm-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: litellm
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?set in .env}
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U litellm"]
      interval: 10s
      timeout: 5s
      retries: 5

  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm
    restart: unless-stopped
    ports:
      - "4000:4000"
    environment:
      # Admin / auth + DB
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:?set in .env}
      DATABASE_URL: postgresql://litellm:${POSTGRES_PASSWORD}@postgres:5432/litellm
      
      # Provider API Keys
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      DEEPSEEK_API_KEY: ${DEEPSEEK_API_KEY:-}
      GOOGLE_API_KEY: ${GOOGLE_API_KEY:-}
      OPENROUTER_API_KEY: ${OPENROUTER_API_KEY:-}
      
      # Additional providers (future)
      COHERE_API_KEY: ${COHERE_API_KEY:-}
      AI21_API_KEY: ${AI21_API_KEY:-}
      REPLICATE_API_KEY: ${REPLICATE_API_KEY:-}
      
      # Enable UI and logging
      LITELLM_UI: "True"
      LITELLM_LOG: INFO
      
      # Optional Redis for distributed caching (uncomment if needed)
      # REDIS_HOST: redis
      # REDIS_PORT: 6379
      # REDIS_PASSWORD: ${REDIS_PASSWORD:-}
    volumes:
      - ./litellm/config-openai.yaml:/app/config.yaml:ro
    depends_on:
      postgres:
        condition: service_healthy
    command: ["--config", "/app/config.yaml", "--host", "0.0.0.0", "--port", "4000"]

  # Local llama.cpp server
  llama:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llama-server
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models:ro
    environment:
      LLAMA_ARG_MODEL: ${LLAMA_MODEL_PATH:-/models/deepseek-coder-v2.gguf}
      LLAMA_ARG_CTX_SIZE: ${LLAMA_CTX_SIZE:-16384}
      LLAMA_ARG_N_PARALLEL: ${LLAMA_N_PARALLEL:-4}
      LLAMA_ARG_PORT: 8080
      LLAMA_ARG_ENDPOINT_METRICS: 1
      LLAMA_ARG_HOST: 0.0.0.0
    # For CUDA support, use the CUDA variant and add GPU resources
    # image: ghcr.io/ggml-org/llama.cpp:server-cuda
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: [gpu]
    #           count: 1

  # Optional: Redis for caching and distributed budget tracking
  # redis:
  #   image: redis:7-alpine
  #   container_name: litellm-redis
  #   restart: unless-stopped
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redisdata:/data
  #   command: redis-server --appendonly yes ${REDIS_PASSWORD:+--requirepass ${REDIS_PASSWORD}}

  # Optional: vLLM server (future)
  # vllm:
  #   image: vllm/vllm-openai:latest
  #   container_name: vllm-server
  #   restart: unless-stopped
  #   ports:
  #     - "8081:8000"
  #   volumes:
  #     - ./models:/models:ro
  #   environment:
  #     MODEL: ${VLLM_MODEL_PATH:-/models/model}
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - capabilities: [gpu]
  #             count: 1

volumes:
  pgdata:
  # redisdata: